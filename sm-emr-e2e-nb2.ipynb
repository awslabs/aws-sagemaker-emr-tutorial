{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Machine Learning Pipeline using SageMaker, EMR, S3, ECS, API Gateway and Lambda \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess data, build, train and deploy a Spark pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by adding few required policies to this notebook instance. Refer \"Step-1: Setup permission\" of [this documentation](https://collaborate-corp.amazon.com/nuxeo/nxpath/default/default-domain/workspaces/ProServe%20Goldemine%20Workspace/Tutorial%3A%20Build%20an%20End%20t@view_documents?tabIds=MAIN_TABS%3Adocuments%2C%3A&conversationId=0NXMAIN1) incase you need help in adding policies to the notebook. Add `AmazonElasticMapReduceFullAccess`, `AmazonSageMakerFullAccess` and `AmazonS3FullAccess` if they're not already added. \n",
    "\n",
    "Now, Let's define few variables. We need a variable to save the name of the s3 bucket which contains the bootstrap scripts and datasets, and another variable to save the name of the notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "\n",
    "os.environ['BUCKET'] =  '<bucket name here>'\n",
    "os.environ['NOTEBOOK_NAME'] =  '<input your notebook instance name here>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we spin the emr spark cluster, we'll save the mleap bootstrap scripts to a s3 bucket which we'll use it while spining up the cluster to install mleap into our clusters. Mleap is a serializer and an execution engine for ML pipelines. Here, we'll train the pipeline in emr spark and export it to mleap bundle. Here's a link if you want to learn more about mleap, https://github.com/combust/mleap\n",
    "\n",
    "We'll extract the *Subnet*, *Security group* details from the current notebook instance. We'll make sure the emr is spun in the same VPC, subnet and security group as this notebook with spark, livy, boto3 and mleap.\n",
    "\n",
    "After the emr cluster is ready, we'll copy the `private IP` of the master node and change it in the notebook's *config.json* file place of 'localhost'. At this point, your notebook should be connected to your EMR.\n",
    "\n",
    "Lastly, we'll save the dataset to the S3 bucket and we'll use it later for model training.\n",
    "\n",
    "Note: If you get \"The supplied ecSubnetId is not valid\" error, this could be due to missing network information while spinning up the SageMaker notebook insatnce. Refer to [this documentation](https://collaborate-corp.amazon.com/nuxeo/nxpath/default/default-domain/workspaces/ProServe%20Goldemine%20Workspace/Tutorial%3A%20Build%20an%20End%20t.1546036770584@view_documents?tabIds=MAIN_TABS%3Adocuments%2C%3A&conversationId=0NXMAIN1) and follow the instructions to set up this notebook instance correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \\#!/bin/bash > emr_model_bootstrap.sh\n",
    "echo sudo pip install boto3 mleap >> emr_model_bootstrap.sh\n",
    "echo wget http://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/jar/mleap_spark_assembly.jar >> emr_model_bootstrap.sh\n",
    "echo sudo mkdir -p /usr/lib/spark/jars >> emr_model_bootstrap.sh\n",
    "echo sudo mv mleap_spark_assembly.jar /usr/lib/spark/jars >> emr_model_bootstrap.sh\n",
    "\n",
    "aws s3 cp emr_model_bootstrap.sh s3://$BUCKET/scripts/\n",
    "\n",
    "# Save bucket info for after restart\n",
    "echo $BUCKET > bucket.tmp\n",
    "\n",
    "# parse the SUBNET, Security group info from the notebook instance\n",
    "SUBNET=$(aws sagemaker describe-notebook-instance --notebook-instance-name $NOTEBOOK_NAME | jq -r .SubnetId)\n",
    "\n",
    "SLAVE_SECURITY_GROUP=$(aws ec2 describe-security-groups --group-names ElasticMapReduce-slave | jq -r .SecurityGroups[0].GroupId)\n",
    "MASTER_SECURITY_GROUP=$(aws ec2 describe-security-groups --group-names ElasticMapReduce-master | jq -r .SecurityGroups[0].GroupId)\n",
    "\n",
    "\n",
    "# Create EMR Cluster\n",
    "CLUSTER_ID=$(aws emr create-cluster --name emr-model-demo --release-label emr-5.10.0 \\\n",
    "    --applications Name=SPARK Name=LIVY  --instance-type m4.xlarge --instance-count 3 \\\n",
    "    --ebs-root-volume-size 100 \\\n",
    "    --ec2-attributes '{\"InstanceProfile\":\"EMR_EC2_DefaultRole\",\"SubnetId\":\"'$SUBNET'\",\"EmrManagedSlaveSecurityGroup\":\"'$SLAVE_SECURITY_GROUP'\",\"EmrManagedMasterSecurityGroup\":\"'$MASTER_SECURITY_GROUP'\"}' \\\n",
    "    --service-role EMR_DefaultRole --bootstrap Path=s3://$BUCKET/scripts/emr_model_bootstrap.sh | jq -r .ClusterId)\n",
    "\n",
    "echo Cluster ID is... $CLUSTER_ID\n",
    "\n",
    "# Wait for Cluster Ready\n",
    "CLUSTER_STATUS=$(aws emr describe-cluster --cluster-id $CLUSTER_ID | jq -r .Cluster.Status.State)\n",
    "while [ $CLUSTER_STATUS != 'WAITING' ]; do\n",
    "  CLUSTER_STATUS=$(aws emr describe-cluster --cluster-id $CLUSTER_ID | jq -r .Cluster.Status.State)\n",
    "  echo $CLUSTER_STATUS\n",
    "  sleep 20\n",
    "done\n",
    "\n",
    "privateip=$(aws emr list-instances --cluster-id $CLUSTER_ID --instance-group-types MASTER | jq -r .Instances[0].PrivateIpAddress)\n",
    "\n",
    "cd .sparkmagic\n",
    "\n",
    "wget https://raw.githubusercontent.com/jupyter-incubator/sparkmagic/master/sparkmagic/example_config.json\n",
    "mv example_config.json config.json\n",
    "\n",
    "sed -i \"s/localhost/$privateip/g\" config.json\n",
    "\n",
    "# download and save the dataset to s3\n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\n",
    "\n",
    "aws s3 cp car.data s3://$BUCKET/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart Notebook\n",
    "\n",
    "After the cluster is up and running in EMR, you need to restart the notebook kernel in order for Sparkmagic to reload the config settings and connect to the cluster over Livy. You can do this by clicking `Kernel` and then `Restart` in the notebook. ***Warning***: You should _not_ run the cells above this point again after restarting the notebook. Just continue from this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Begin by running the following cell to confirm that this notebook is connected to EMR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the bucket and notebook names from the temporary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "\n",
    "# Obtain the bucket from the temporary file\n",
    "with open('bucket.tmp', 'r') as f:\n",
    "    os.environ['BUCKET'] = f.read().replace('\\n', '')\n",
    "    os.environ['NOTEBOOK_NAME'] = f.read().replace('\\n', '') #'podk-sm-emr'\n",
    "\n",
    "print('Please set this value in the next cell in order to send the bucket to the Spark cluster:\\n{}'.format(os.environ['BUCKET']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you've your bucket name here, set the same bucket name again in spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = '<bucket name here>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import neccessary dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "import boto3\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoder, VectorAssembler, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've the s3 bucket name saved, we'll define few s3 prefixes inside this bucket. One for input data location where the dataset will reside, one for an output bucket prefix where output of inference will be saved and one for model bucket prefix where model artifacts will be saved. \n",
    "\n",
    "We'll start with reading the data from S3. In the car dataset, the target variable i.e. car acceptibility is a string. We'll convert it to a number and split the data into train and validate dataset. We'll then build a random forest model and a pipeline with preprocess steps and the model. Let's use this pipeline to train the model. \n",
    "\n",
    "Save the transformed training and validation data to two separate CSVs in S3.\n",
    "\n",
    "Serialize the trained model and store via Mleap bundle, then convert it to .tar.gz file since SageMaker expects that format. We'll then do the same for postprocess steps. We'll use an inference pipeline for inference on data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toCSVLine(data):\n",
    "    r = ','.join(str(d) for d in data[1])\n",
    "    return str(data[0]) + \",\" + r\n",
    "\n",
    "\n",
    "# Set S3 bucket locations as variables\n",
    "args = {\n",
    "    's3_input_data_location' : 's3://{}/data/car.data'.format(bucket_name), \n",
    "    's3_output_bucket': bucket_name,  \n",
    "    's3_output_bucket_prefix': 'output' , \n",
    "    's3_model_bucket' : bucket_name,  \n",
    "    's3_model_bucket_prefix' : 'model'}\n",
    "\n",
    "\n",
    "# This is needed to write RDDs to file which is the only way to write nested Dataframes into CSV.\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"mapred.output.committer.class\",\n",
    "                                                  \"org.apache.hadoop.mapred.FileOutputCommitter\")\n",
    "\n",
    "train = spark.read.csv(args['s3_input_data_location'], header=False)\n",
    "\n",
    "\n",
    "oldColumns = train.schema.names\n",
    "newColumns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'cat']\n",
    "\n",
    "train = reduce(lambda train, idx: train.withColumnRenamed(oldColumns[idx], newColumns[idx]), xrange(len(oldColumns)), train)\n",
    "\n",
    "# dropping null values\n",
    "train = train.dropna()\n",
    "\n",
    "# Target label\n",
    "catIndexer = StringIndexer(inputCol=\"cat\", outputCol=\"label\")\n",
    "\n",
    "labelIndexModel = catIndexer.fit(train)\n",
    "train = labelIndexModel.transform(train)\n",
    "\n",
    "converter = IndexToString(inputCol=\"label\", outputCol=\"cat\")\n",
    "\n",
    "# Spliting in train and test set. Beware : It sorts the dataset\n",
    "(traindf, validationdf) = train.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "buyingIndexer = StringIndexer(inputCol=\"buying\", outputCol=\"indexedBuying\")\n",
    "maintIndexer = StringIndexer(inputCol=\"maint\", outputCol=\"indexedMaint\")\n",
    "doorsIndexer = StringIndexer(inputCol=\"doors\", outputCol=\"indexedDoors\")\n",
    "personsIndexer = StringIndexer(inputCol=\"persons\", outputCol=\"indexedPersons\")\n",
    "lug_bootIndexer = StringIndexer(inputCol=\"lug_boot\", outputCol=\"indexedLug_boot\")\n",
    "safetyIndexer = StringIndexer(inputCol=\"safety\", outputCol=\"indexedSafety\")\n",
    "\n",
    "\n",
    "# One Hot Encoder on indexed features\n",
    "buyingEncoder = OneHotEncoder(inputCol=\"indexedBuying\", outputCol=\"buyingVec\")\n",
    "maintEncoder = OneHotEncoder(inputCol=\"indexedMaint\", outputCol=\"maintVec\")\n",
    "doorsEncoder = OneHotEncoder(inputCol=\"indexedDoors\", outputCol=\"doorsVec\")\n",
    "personsEncoder = OneHotEncoder(inputCol=\"indexedPersons\", outputCol=\"personsVec\")\n",
    "lug_bootEncoder = OneHotEncoder(inputCol=\"indexedLug_boot\", outputCol=\"lug_bootVec\")\n",
    "safetyEncoder = OneHotEncoder(inputCol=\"indexedSafety\", outputCol=\"safetyVec\")\n",
    "\n",
    "\n",
    "# Create the vector structured data (label,features(vector))\n",
    "assembler = VectorAssembler(inputCols=[\"buyingVec\", \"maintVec\", \"doorsVec\", \"personsVec\", \"lug_bootVec\", \"safetyVec\"], outputCol=\"features\")\n",
    "\n",
    "# rf model \n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "# Chain featurizers in a Pipeline\n",
    "pipeline = Pipeline(stages=[buyingIndexer, maintIndexer, doorsIndexer, personsIndexer, lug_bootIndexer, safetyIndexer, buyingEncoder, maintEncoder, doorsEncoder, personsEncoder, lug_bootEncoder, safetyEncoder, assembler, rf])\n",
    "\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(traindf)\n",
    "\n",
    "# Delete previous data from output\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(args['s3_output_bucket'])\n",
    "\n",
    "bucket.objects.filter(Prefix=args['s3_output_bucket_prefix']).delete()    \n",
    "\n",
    "# Save transformed training data to CSV in S3 by converting to RDD.\n",
    "transformed_traindf = model.transform(traindf)\n",
    "transformed_train_rdd = transformed_traindf.rdd.map(lambda x: (x.label, x.features))\n",
    "lines = transformed_train_rdd.map(toCSVLine)\n",
    "lines.saveAsTextFile('s3a://' + args['s3_output_bucket'] + '/' +args['s3_output_bucket_prefix'] + '/' + 'train')\n",
    "\n",
    "# Similar data processing for validation dataset.\n",
    "predictions = model.transform(validationdf)\n",
    "transformed_train_rdd = predictions.rdd.map(lambda x: (x.label, x.features))\n",
    "lines = transformed_train_rdd.map(toCSVLine)\n",
    "lines.saveAsTextFile('s3a://' + args['s3_output_bucket'] + '/' +args['s3_output_bucket_prefix'] + '/' + 'validation')\n",
    "\n",
    "# Serialize and store via MLeap  \n",
    "SimpleSparkSerializer().serializeToBundle(model, \"jar:file:/tmp/model.zip\", predictions)\n",
    "\n",
    "# Unzipping as SageMaker expects a .tar.gz file but MLeap produces a .zip file.\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"/tmp/model.zip\") as zf:\n",
    "    zf.extractall(\"/tmp/model\")\n",
    "\n",
    "## Writing back the content as a .tar.gz file\n",
    "import tarfile\n",
    "with tarfile.open(\"/tmp/model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"/tmp/model/bundle.json\", arcname='bundle.json')\n",
    "    tar.add(\"/tmp/model/root\", arcname='root')\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "file_name = args['s3_model_bucket_prefix'] + '/' + 'model.tar.gz'\n",
    "s3.Bucket(args['s3_model_bucket']).upload_file('/tmp/model.tar.gz', file_name)\n",
    "\n",
    "os.remove('/tmp/model.zip')\n",
    "os.remove('/tmp/model.tar.gz')\n",
    "shutil.rmtree('/tmp/model')\n",
    "\n",
    "# Save postprocessor          \n",
    "SimpleSparkSerializer().serializeToBundle(converter, \"jar:file:/tmp/postprocess.zip\", predictions)\n",
    "\n",
    "with zipfile.ZipFile(\"/tmp/postprocess.zip\") as zf:\n",
    "    zf.extractall(\"/tmp/postprocess\")\n",
    "\n",
    "# Writing back the content as a .tar.gz file\n",
    "import tarfile\n",
    "with tarfile.open(\"/tmp/postprocess.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"/tmp/postprocess/bundle.json\", arcname='bundle.json')\n",
    "    tar.add(\"/tmp/postprocess/root\", arcname='root')\n",
    "\n",
    "file_name = args['s3_model_bucket_prefix'] + '/' + 'postprocess.tar.gz'\n",
    "s3.Bucket(args['s3_model_bucket']).upload_file('/tmp/postprocess.tar.gz', file_name)\n",
    "\n",
    "os.remove('/tmp/postprocess.zip')\n",
    "os.remove('/tmp/postprocess.tar.gz')\n",
    "shutil.rmtree('/tmp/postprocess')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sagemaker session and create sagemaker endpoint with pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "from sagemaker import Session as Sess\n",
    "\n",
    "# SageMaker session\n",
    "sess = Sess()\n",
    "\n",
    "# Boto3 session   \n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "### Create SageMaker endpoint with pipeline\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "\n",
    "bucket_name = os.environ['BUCKET']\n",
    "\n",
    "# Image locations are published at: https://github.com/aws/sagemaker-sparkml-serving-container\n",
    "sparkml_images = {\n",
    "    'us-west-1': '746614075791.dkr.ecr.us-west-1.amazonaws.com/sagemaker-sparkml-serving:2.2',\n",
    "    'us-west-2': '246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-sparkml-serving:2.2',\n",
    "    'us-east-1': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sparkml-serving:2.2',\n",
    "    'us-east-2': '257758044811.dkr.ecr.us-east-2.amazonaws.com/sagemaker-sparkml-serving:2.2',\n",
    "    'ap-northeast-1': '354813040037.dkr.ecr.ap-northeast-1.amazonaws.com/sagemaker-sparkml-serving:2.2',\n",
    "    'ap-northeast-2': '366743142698.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-sparkml-serving:2.2',\n",
    "    'ap-southeast-1': '121021644041.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker-sparkml-serving:2.2',\n",
    "    'ap-southeast-2': '783357654285.dkr.ecr.ap-southeast-2.amazonaws.com/sagemaker-sparkml-serving:2.2',\n",
    "    'ap-south-1': '720646828776.dkr.ecr.ap-south-1.amazonaws.com/sagemaker-sparkml-serving:2.2',\n",
    "    'eu-west-1': '141502667606.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-sparkml-serving:2.2',\n",
    "    'eu-west-2': '764974769150.dkr.ecr.eu-west-2.amazonaws.com/sagemaker-sparkml-serving:2.2',\n",
    "    'eu-central-1': '492215442770.dkr.ecr.eu-central-1.amazonaws.com/sagemaker-sparkml-serving:2.2',\n",
    "    'ca-central-1': '341280168497.dkr.ecr.ca-central-1.amazonaws.com/sagemaker-sparkml-serving:2.2',\n",
    "    'us-gov-west-1': '414596584902.dkr.ecr.us-gov-west-1.amazonaws.com/sagemaker-sparkml-serving:2.2'\n",
    "}\n",
    "\n",
    "model_data_url_0 = 's3://{}/model/model.tar.gz'.format(bucket_name)\n",
    "model_data_url_1 = 's3://{}/model/postprocess.tar.gz'.format(bucket_name)\n",
    "\n",
    "try:\n",
    "    sparkml_image = sparkml_images[region]\n",
    "\n",
    "    response = sagemaker.create_model(\n",
    "        ModelName='pipeline-rf',\n",
    "        Containers=[\n",
    "            {\n",
    "                'Image': sparkml_image,\n",
    "                'ModelDataUrl': model_data_url_0,\n",
    "                'Environment': {\n",
    "                    'SAGEMAKER_SPARKML_SCHEMA': '{\"input\":[{\"type\":\"string\",\"name\":\"buying\"},{\"type\":\"string\",\"name\":\"maint\"},{\"type\":\"string\",\"name\":\"doors\"},{\"type\":\"string\",\"name\":\"persons\"},{\"type\":\"string\",\"name\":\"lug_boot\"},{\"type\":\"string\",\"name\":\"safety\"}],\"output\":{\"type\":\"double\",\"name\":\"features\",\"struct\":\"vector\"}}'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'Image': sparkml_image,\n",
    "                'ModelDataUrl': model_data_url_1,\n",
    "                'Environment': {\n",
    "                    'SAGEMAKER_SPARKML_SCHEMA': '{\"input\": [{\"type\": \"double\", \"name\": \"label\"}], \"output\": {\"type\": \"string\", \"name\": \"cat\"}}'\n",
    "                }\n",
    "\n",
    "            },\n",
    "        ],\n",
    "        ExecutionRoleArn=role\n",
    "    )\n",
    "\n",
    "    print('{}\\n'.format(response))\n",
    "    \n",
    "except ClientError as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "try:\n",
    "    response = sagemaker.create_endpoint_config(\n",
    "        EndpointConfigName='pipeline-rf',\n",
    "        ProductionVariants=[\n",
    "            {\n",
    "                'VariantName': 'DefaultVariant',\n",
    "                'ModelName': 'pipeline-rf',\n",
    "                'InitialInstanceCount': 1,\n",
    "                'InstanceType': 'ml.m4.xlarge',\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    print('{}\\n'.format(response))\n",
    "\n",
    "except ClientError as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "try:\n",
    "    response = sagemaker.create_endpoint(\n",
    "        EndpointName='pipeline-rf',\n",
    "        EndpointConfigName='pipeline-rf',\n",
    "    )\n",
    "    print('{}\\n'.format(response))\n",
    "\n",
    "except ClientError as e:\n",
    "    print(e)\n",
    "\n",
    "import time\n",
    "    \n",
    "\n",
    "# Monitor the status until completed\n",
    "endpoint_status = sagemaker.describe_endpoint(EndpointName='pipeline-rf')['EndpointStatus']\n",
    "while endpoint_status not in ('OutOfService','InService','Failed'):\n",
    "    endpoint_status = sagemaker.describe_endpoint(EndpointName='pipeline-rf')['EndpointStatus']\n",
    "    print(endpoint_status)\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model endpoint is created successfully, we can start predicting. Let's test this with a sample payload eg: ['vhigh','vhigh',2,2,'small','low']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName='pipeline-rf',\n",
    "    Body=b'vhigh,vhigh,2,2,small,low',\n",
    "    ContentType='text/csv',\n",
    ")\n",
    "\n",
    "print(response['Body'].read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! We've successfully trained a model in EMR spark, and deployed it in sagemaker endpoint with an inference pipeline. Using the sagemaker endpoint we were able to run inference. \n",
    "\n",
    "Now, let's delete all the resources we created for this excercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "os.environ['SAGEMAKER_MODEL_NAME'] = 'pipeline-rf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "aws sagemaker delete-endpoint --endpoint-name $SAGEMAKER_MODEL_NAME\n",
    "aws sagemaker delete-endpoint-config --endpoint-config-name $SAGEMAKER_MODEL_NAME\n",
    "aws sagemaker delete-model --model-name $SAGEMAKER_MODEL_NAME\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
